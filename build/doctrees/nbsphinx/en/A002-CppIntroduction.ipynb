{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2 - Introduction to OpenJij core interface (C++ interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OpenJij/OpenJijTutorial/blob/master/source/en/A002-CppIntroduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we introduce core interface (C++ interface) of OpenJij. If you only want to deal with the core Python interface, you can skip this section.  \n",
    "\n",
    "C++ interface is the lowest layer API of OpenJij. Core Python interface read out C++ interface internally. It can be used in applications where we want to get the best performance out of OpenJij using only C++, but not Python. \n",
    "\n",
    "## Run a problem\n",
    "\n",
    "First, we clone repository from github.\n",
    "\n",
    "```sh\n",
    "$ git clone https://github.com/OpenJij/OpenJij\n",
    "$ cd OpenJij\n",
    "```\n",
    "\n",
    "OpenJij is essentially a header-only library. Hence, we only need to specify the `src` directory path at compile to be able to use the C++ interface. We need to build library with CMake if we want to use GPU algorithms. Executing `build_gcc.sh` allows you to build automatically.\n",
    "\n",
    "The code that output exactly the same result as the Python interface in the previous section can be written as follows (The same description can be found in `project_template/template.cpp`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "#include <graph/all.hpp>\n",
    "#include <system/all.hpp>\n",
    "#include <updater/all.hpp>\n",
    "#include <algorithm/all.hpp>\n",
    "#include <result/all.hpp>\n",
    "#include <utility/schedule_list.hpp>\n",
    "#include <random>\n",
    "\n",
    "#include <iostream>\n",
    "\n",
    "using namespace openjij;\n",
    "\n",
    "int main(void){\n",
    "\n",
    "    // set interaction matrix. use Graph module\n",
    "    // define Dense graph of problem size N=5\n",
    "    constexpr std::size_t N = 5;\n",
    "    auto dense = graph::Dense<double>(N);\n",
    "\n",
    "    // set interaction\n",
    "    for(std::size_t i=0; i<N; i++){\n",
    "        for(std::size_t j=0; j<N; j++){\n",
    "            dense.J(i, j) = (i == j) ? 0 : -1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // set longitudinal magnetic fields\n",
    "    for(std::size_t i=0; i<N; i++){\n",
    "        dense.h(i) = -1;\n",
    "    }\n",
    "\n",
    "    // define random number generator (Mersenne Twister)\n",
    "    auto rand_engine = std::mt19937(0x1234);\n",
    "\n",
    "    // set System to compute from Graph\n",
    "    // use system of normal classical MonteCarlo calculation\n",
    "    auto system = system::make_classical_ising(dense.gen_spin(rand_engine), dense);\n",
    "\n",
    "    // set schedules of annealing. use Utility module\n",
    "    auto schedule_list = utility::make_classical_schedule_list(0.1, 100, 10, 10);\n",
    "\n",
    "    // execute annealing. use Algorithm module\n",
    "    // use simple SingleSpinFlip as a updating MonteCarlo step\n",
    "    algorithm::Algorithm<updater::SingleSpinFlip>::run(system, rand_engine, schedule_list);\n",
    "\n",
    "    // get result\n",
    "    std::cout << \"The result spins are [\";\n",
    "    for(auto&& elem : result::get_solution(system)){\n",
    "        std::cout << elem << \" \";\n",
    "    }\n",
    "\n",
    "    std::cout << \"]\" << std::endl;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At First, we execute `make` in `project_template`, and second we execute `./tutorial`. Then we can see that $[1, 1, 1, 1, 1]$ is output as a solution just as before.   \n",
    "For more details of compile option, see also `Makefile` in `projecto_template`. Especially, when we use altorithms on GPUs, we can use `build_gcc.sh` to build them. Note that we need to link the CUDA library.\n",
    "\n",
    "Comparison with sample script of core Python interface, it can be written in much the same way as a Python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
